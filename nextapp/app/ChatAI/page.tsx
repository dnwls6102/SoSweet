'use client';

import React, { useEffect, useState, useRef } from 'react';
import { useRouter } from 'next/navigation';
import styles from './page.module.css';
import { useDispatch } from 'react-redux';
import { setSummary, setConclusion } from '@/store/feedbackSlice';
import Cookies from 'js-cookie';
import { jwtDecode } from 'jwt-decode';
import { setIsAIChat } from '@/store/aiFlagSlice';
import Image from 'next/image';

interface UserPayload {
  user_id: string;
  user_gender: string;
  iat: number;
  exp: number;
}

// í”„ë ˆì„ ì¹´ìš´í„° ì¶”ê°€
let frameCounter = 0;

interface SpeechRecognitionEvent {
  resultIndex: number;
  results: {
    [key: number]: {
      [key: number]: {
        transcript: string;
        isFinal?: boolean;
      };
      isFinal?: boolean;
      length: number;
    };
    length: number;
  };
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  lang: string;
  start(): void;
  stop(): void;
  onstart: () => void;
  onend: () => void;
  onerror: (event: { error: string }) => void;
  onresult: (event: SpeechRecognitionEvent) => void;
  onspeechstart: () => void;
  onspeechend: () => void;
  state?: string;
}

interface Window {
  webkitSpeechRecognition: new () => SpeechRecognition;
}

export default function Chat() {
  const router = useRouter();
  const [user_id, setUserId] = useState('');
  const [user_gender, setUserGender] = useState('');

  useEffect(() => {
    const token = Cookies.get('access');
    if (token) {
      const decoded = jwtDecode<UserPayload>(token);
      setUserId(decoded.user_id);
      setUserGender(decoded.user_gender);
    } else {
      alert('ìœ íš¨í•˜ì§€ ì•Šì€ ì ‘ê·¼ì…ë‹ˆë‹¤.');
      router.replace('/');
    }
  }, [router]);

  const image_src = user_gender === 'ë‚¨ì„±' ? '/emma.webp' : '/john.webp';

  const localVideoRef = useRef<HTMLVideoElement>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const isRecording = useRef<boolean>(false);
  const [feedback, setFeedback] = useState('');
  const scriptRef = useRef('');
  const recognition = useRef<SpeechRecognition | null>(null);

  const [recordedChunks, setRecordedChunks] = useState<Blob[]>([]); // ë…¹í™” ë°ì´í„° ìŒ“ëŠ” ë°°ì—´
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);

  // ê°ì • ë¶„ì„ì„ ìœ„í•œ ìƒíƒœ ì¶”ê°€
  const [myEmotion, setMyEmotion] = useState('ğŸ˜¶ í‰ì˜¨í•¨');
  const [myValue, setMyValue] = useState(0);

  const dispatch = useDispatch();
  // const [relationshipScore, setRelationshipScore] = useState(48);

  //ëŒ€í™” ì˜ìƒ ì „ì²´ / në¶„ ê°„ê²©ìœ¼ë¡œ ì„œë²„ë¡œ ë³´ë‚´ëŠ” í•¨ìˆ˜

  const tryNlp = async (script: string) => {
    try {
      const response = await fetch(
        `${process.env.NEXT_PUBLIC_API_URL}/api/nlp`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({ script }),
        },
      );
      if (response.ok) {
        const result = await response.json();
        console.log(result.message);
        setFeedback((prev) =>
          prev ? prev + '\n' + result.message : result.message,
        );
      } else {
        const result = await response.json();
        console.log(result.error);
        setFeedback((prev) => (prev ? prev + result.message : result.message));
      }
    } catch (error) {
      console.log('ì„œë²„ ì˜¤ë¥˜ ë°œìƒ : ', error);
    }
  };

  const trySendScript = async (script: string) => {
    const emotion = { emotion: myEmotion, value: myValue };
    try {
      const response = await fetch(
        `${process.env.NEXT_PUBLIC_SERVER_URL}/api/ai/dialog`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          credentials: 'include',
          body: JSON.stringify({ script, user_id, user_gender, emotion }),
        },
      );

      if (response.ok) {
        const audioBlob = await response.blob(); // ì„œë²„ ì‘ë‹µ ë°ì´í„°ë¥¼ Blobìœ¼ë¡œ ë³€í™˜
        const audioUrl = URL.createObjectURL(audioBlob); // Blobì—ì„œ ì¬ìƒ ê°€ëŠ¥í•œ URL ìƒì„±
        const audio = new Audio(audioUrl); // Audio ê°ì²´ ìƒì„±
        isRecording.current = false;
        recognition.current?.stop();
        audio.addEventListener('ended', () => {
          console.log('ìŒì„± ì¬ìƒ ì™„ë£Œ');

          isRecording.current = true;
          recognition.current?.start();
        });
        audio.play(); // ìŒì„± íŒŒì¼ ì¬ìƒ
        console.log('ì „ì†¡ ì„±ê³µ');
      } else {
        console.log('ì˜¤ë¥˜ ë°œìƒ');
      }
    } catch (error) {
      console.log('ì„œë²„ ì˜¤ë¥˜ ë°œìƒ : ', error);
    }
  };

  const handleStartRecording = () => {
    if (!recognition.current) return;

    recognition.current.onstart = () => {
      isRecording.current = true;
      console.log('Speech recognition started');
    };

    recognition.current.onspeechstart = () => {
      console.log('ì‚¬ìš©ìê°€ ë§ì„ ì‹œì‘í•¨');
    };

    //onresult : ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ë°œìƒí• ë•Œë§ˆë‹¤ í˜¸ì¶œë¨
    recognition.current.onresult = (event: SpeechRecognitionEvent) => {
      for (let i = event.resultIndex; i < event.results.length; i++) {
        if (event.results[i].isFinal) {
          scriptRef.current += event.results[i][0].transcript;
        }
      }
      console.log('Transcription result: ', scriptRef.current);
      if (scriptRef.current !== '') {
        tryNlp(scriptRef.current);
        trySendScript(scriptRef.current);
        //ì—¬ê¸°ì—ë‹¤ê°€ ìŒì„± ì¸ì‹ ê²°ê³¼ë¥¼ ë³´ë‚¼ ê²½ìš° : ë³€í™˜ ê²°ê³¼ë¥¼ ë°”ë¡œë°”ë¡œ ë³´ë‚´ì£¼ê¸° ë•Œë¬¸ì—
        //ë§ì„ ëŠì—ˆëŠ”ì§€ ì—¬ë¶€ë„ ì¡°ê¸ˆ ë” ëª…í™•í•˜ê²Œ íŒë‹¨ ê°€ëŠ¥í• ìˆ˜ë„ ìˆë‹¤
        //ì´ë ‡ê²Œ ë˜ë©´ ë‚¨,ë…€ êµ¬ë¶„ì€ ì–´ë–»ê²Œ í•´ì•¼í• ì§€?
        //ì„œë²„ì—ì„œ ë¬¸ìì—´ì„ ë°›ì„ ë•Œë§ˆë‹¤ (ë‚¨)í‘œì‹œë¥¼ í•˜ë©´ scriptê°€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë ë ¨ì§€
        //ë¶€í•˜ê°€ ë§ì´ ê±¸ë¦¬ëŠ”ì§€? ì‹¤ì œë¡œ ë§Œì¡±ìŠ¤ëŸ¬ìš¸ ì •ë„ë¡œ í†µì‹ ì´ ë ë ¨ì§€ëŠ” ìƒê°í•´ë´ì•¼ í•¨
        scriptRef.current = '';
      }
    };

    recognition.current.onspeechend = () => {
      console.log('onspeechend called');
      console.log('Final transcript:', scriptRef.current);
      /* ì´ê³³ì— trySendScriptë¡œ ë°œí™” ë‚´ìš©ì„ ì „ì†¡í•˜ë©´
         ë¬¸ì œì  : ì‚¬ìš©ìê°€ ë°œí™”í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨í•˜ëŠ” ì‹œê°„ì„ ë„ˆë¬´ ë³´ìˆ˜ì ìœ¼ë¡œ ì¡ìŒ
         --> chunk ë‚´ìš©ì´ ì—„ì²­ ê¸¸ì–´ì§€ê²Œ ë¨
      */
      // scriptRef.current = '';
    };

    recognition.current.onend = () => {
      console.log('Speech recognition session ended.');
      console.log('isRecording:', isRecording.current);
      if (isRecording.current) {
        console.log('Restarting speech recognition...');
        recognition.current?.start();
      }
    };

    recognition.current.onerror = (event: { error: string }) => {
      if (event.error !== 'no-speech')
        console.error('Speech recognition error:', event.error);
    };

    recognition.current.start();
  };

  useEffect(() => {
    if (!('webkitSpeechRecognition' in window)) {
      alert('ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.');
      return;
    }

    recognition.current = new (
      window as unknown as Window
    ).webkitSpeechRecognition();
    recognition.current.lang = 'ko';
    recognition.current.continuous = true;

    handleStartRecording();

    const initializeMedia = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: true,
        });

        mediaStreamRef.current = stream;

        // ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ì— ìŠ¤íŠ¸ë¦¼ ì—°ê²°
        if (localVideoRef.current) {
          localVideoRef.current.srcObject = stream;
          console.log('ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì—°ê²° ì„±ê³µ');
        }

        // ì—¬ê¸°ì„œ MediaRecorderë¡œ 'ì „ì²´ ì˜ìƒ Blob' ì €ì¥ ë¡œì§ êµ¬í˜„í•˜ê¸°
        setupMediaRecorder(stream);
      } catch (err) {
        console.error('Error accessing media devices:', err);
      }
    };

    const setupMediaRecorder = (stream: MediaStream) => {
      const recorder = new MediaRecorder(stream, {
        mimeType: 'video/webm; codecs=vp8',
      });
      mediaRecorderRef.current = recorder;
      console.log('MediaRecorder ì„¤ì • ì™„ë£Œ');

      recorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          // Blob ì¡°ê° ìŒ“ì•„ë‘ê¸°
          setRecordedChunks((prev) => [...prev, event.data]);
          // console.log('ë…¹í™” ë°ì´í„° ì²­í¬ ì €ì¥ë¨');
        }
      };

      recorder.onstop = () => {
        console.log('ë…¹í™” ì¤‘ì§€ë¨. recordedChunks:', recordedChunks);
      };

      // ë…¹í™” ì‹œì‘
      recorder.start(1000);
      console.log('ë…¹í™” ì‹œì‘ë¨');
    };

    initializeMedia();

    // Canvasë¡œ ë™ì˜ìƒ ì´ë¯¸ì§€ë¡œ ìº¡ì³í•˜ê³  ì„œë²„ë¡œ ì „ì†¡í•˜ê¸°
    const captureAndSendFrame = async () => {
      const videoEl = localVideoRef.current;
      if (!videoEl) {
        console.log('ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ');
        return;
      }

      if (!videoEl.srcObject) {
        console.log('ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì´ ì—†ìŒ');
        return;
      }

      // í˜„ì¬ ë¹„ë””ì˜¤ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
      const vWidth = videoEl.videoWidth / 4;
      const vHeight = videoEl.videoHeight / 4;

      // ì˜ìƒ ì•„ì§ ì¤€ë¹„ ì•ˆ ë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µí•˜ê¸°
      if (!vWidth || !vHeight) {
        console.log('ë¹„ë””ì˜¤ í¬ê¸°ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ');
        return;
      }

      // canvas ìƒì„±í•˜ê¸°
      const canvas = document.createElement('canvas');
      canvas.width = vWidth;
      canvas.height = vHeight;

      const ctx = canvas.getContext('2d');
      if (!ctx) return;

      // canvasì— ë¹„ë””ì˜¤ ê·¸ë¦¬ê¸°
      ctx.drawImage(videoEl, 0, 0, canvas.width, canvas.height);

      // JPEG í¬ë§·ìœ¼ë¡œ Base64 ì¸ì½”ë”©
      const dataURL = canvas.toDataURL('image/jpeg', 0.7);

      frameCounter += 1;

      // í˜„ì¬ ì‹œê°„ìœ¼ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„
      const timestamp = frameCounter;

      try {
        // AI ì±„íŒ…ìš© ê°ì • ë¶„ì„ ìš”ì²­
        const response = await fetch(
          `${process.env.NEXT_PUBLIC_API_URL}/api/ai/frameInfo`,
          {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              frame: dataURL,
              timestamp: timestamp,
              user_id: user_id,
            }),
            mode: 'cors',
          },
        );

        if (!response.ok) {
          console.error('ê°ì • ë¶„ì„ ì‘ë‹µ ì—ëŸ¬:', response.status);
          return;
        }

        // ì„œë²„ì—ì„œ ë°›ì€ ê°ì • ë¶„ì„ ê²°ê³¼
        const analyzeResult = await response.json();
        console.log('ê°ì • ë¶„ì„ ê²°ê³¼:', analyzeResult);

        // ê°ì • ìƒíƒœ ì—…ë°ì´íŠ¸
        if (analyzeResult.dominant_emotion) {
          setMyEmotion(analyzeResult.dominant_emotion);
          setMyValue(analyzeResult.value);
        }
      } catch (error) {
        console.error('ì „ì†¡ ì—ëŸ¬:', error);
      }
    };

    // ì¼ì • ê°„ê²©ìœ¼ë¡œ ìº¡ì²˜ ë° ì „ì†¡
    const intervalId = setInterval(captureAndSendFrame, 1500);

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    return () => {
      // ì¸í„°ë²Œ ì •ë¦¬
      clearInterval(intervalId);

      // ìŒì„± ì¸ì‹ ì •ë¦¬
      if (recognition.current) {
        recognition.current.stop();
        isRecording.current = false;
      }

      console.log('Unmounting...');

      // MediaRecorder ì •ë¦¬
      if (
        mediaRecorderRef.current &&
        mediaRecorderRef.current.state !== 'inactive'
      ) {
        try {
          mediaRecorderRef.current.stop();
          console.log('MediaRecorder ì •ì§€ë¨');
          mediaRecorderRef.current = null;
        } catch (err) {
          console.error('MediaRecorder ì •ì§€ ì¤‘ ì˜¤ë¥˜:', err);
        }
      }

      // ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach((track) => {
          track.stop();
          console.log('ë¹„ë””ì˜¤ íŠ¸ë™ ì •ì§€ë¨');
        });
        mediaStreamRef.current = null;
      }

      // ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ì •ë¦¬
      if (localVideoRef.current) {
        localVideoRef.current.srcObject = null;
      }

      // ë…¹í™” ë°ì´í„° ì´ˆê¸°í™”
      setRecordedChunks([]);
    };
  }, []);

  const handleNavigation = async () => {
    try {
      // ë¨¼ì € ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬
      if (
        mediaRecorderRef.current &&
        mediaRecorderRef.current.state !== 'inactive'
      ) {
        mediaRecorderRef.current.stop();
        mediaRecorderRef.current = null;
      }

      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach((track) => {
          track.stop();
        });
        mediaStreamRef.current = null;
      }

      const response = await fetch(
        `${process.env.NEXT_PUBLIC_SERVER_URL}/api/ai/dialog/end`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({ script: 'end', user_id }),
          credentials: 'include',
        },
      );

      if (response.ok) {
        const json_data = await response.json();
        console.log(typeof json_data.analysis);
        console.log(json_data.analysis);
        const data = JSON.parse(json_data.analysis);
        console.log(data.analysis.analysis);
        console.log(data.analysis.conclusion);
        dispatch(setSummary(data.analysis));
        dispatch(setConclusion(data.conclusion));
        dispatch(setIsAIChat(true));

        // router.push ëŒ€ì‹  window.location.href ì‚¬ìš©
        router.push('/Feedback');
        // window.location.href = '/Feedback';
      } else {
        console.log('ëŒ€í™” ì¢…ë£Œ ìš”ì²­ ì‹¤íŒ¨');
      }
    } catch (error) {
      console.error('ëŒ€í™” ì¢…ë£Œ ìš”ì²­ ì˜¤ë¥˜:', error);
    }
  };

  return (
    <div className={styles.wrapper}>
      <div className={styles.content}>
        <div className={styles.left}>
          <div className={styles.relationshipContainer}>
            <div className={styles.relationshipSet}>
              <Image
                src="/heart.svg"
                alt="í•˜íŠ¸"
                width={24}
                height={24}
                className={styles.heartIcon}
              />
              <div className={styles.relationshipExp}>
                <div className={styles.progressBar}>
                  <div
                    className={styles.progressFill}
                    style={{ width: `${48}%` }}
                  >
                    <span className={styles.progressValue}>{48}/100</span>
                  </div>
                </div>
              </div>
            </div>
            <Image
              src={image_src}
              alt="AI ì´ë¯¸ì§€"
              width={800}
              height={500}
              className={!isRecording ? styles.imageBorderActive : ''}
            />
            <Image
              className={styles.callEndIcon}
              onClick={handleNavigation}
              src="/call-end.svg"
              alt="ëŒ€í™” ì¢…ë£Œ"
              width={50}
              height={50}
            />
          </div>
        </div>
        <div className={styles.right}>
          <h2 className={styles.title}>ë‹¹ì‹ ì˜ ì–¸ì–´ì  ìŠµê´€</h2>
          <textarea
            className={styles.textareaVerbal}
            readOnly
            value={feedback}
          ></textarea>
          <video
            ref={localVideoRef}
            autoPlay
            playsInline
            muted
            style={{ width: 1, height: 1 }}
            // style={{ display: 'none' }}
          />
          <h2 className={styles.title}>ë‹¹ì‹ ì˜ í˜„ì¬ ê°ì •</h2>
          <textarea
            className={styles.textareaEmotion}
            readOnly
            value={myEmotion}
          ></textarea>
        </div>
      </div>
    </div>
  );
}
